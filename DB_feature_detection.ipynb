{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9fcf4f5-8c3b-4d92-9ad1-d6910b85c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2, argparse\n",
    "import math\n",
    "\n",
    "# pytorch modules\n",
    "import torch \n",
    "from torch.utils.data import Dataset # primitive for the data\n",
    "from torch.utils.data import WeightedRandomSampler, DataLoader # wraps the data so its iterable\n",
    "from torch import nn # nn class our model inherits from\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdb61ad-7618-4215-a55c-c2316c4a071f",
   "metadata": {},
   "source": [
    "#### This data is from Silicon that has had a single layer of Hydrogen deposited on top. Then, some of the Hydrogen is desorbed, leaving Silicon dangling bonds. We then dose this with Arsine and Arsenic bonds to these dangling bonds. Once bonded, it has a 100% chance of incorporation after heating. \n",
    "\n",
    "#### After Arsenic has been introduce to the Hydrogen surface, there should be no DBs left as we try to terminate them all, but there may be some left. So, we need to include DBs in this CNN. They should look the same, so we will use some of the data already labelled that was used to train the single and double DB CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1956623-5491-4170-afd0-0ab3d8c3e0ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filled_gr = np.load(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\undosed\\numpy arrays\\20181015-142255_STM_AtomManipulation-Gloucester Road-Si(100)-H--9_1_0.npy')[::2,::2]\n",
    "empty_gr = np.load(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\undosed\\numpy arrays\\20181015-142255_STM_AtomManipulation-Gloucester Road-Si(100)-H--9_1_1_cor.npy')[::2,::2]\n",
    "filled_ec = np.load(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\undosed\\numpy arrays\\20181019-110413_STM_AtomManipulation-earls court-Si(100)-H--63_1_0.npy')[::-1,:][::2,::2]\n",
    "empty_ec = np.load(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\undosed\\numpy arrays\\20181019-110413_STM_AtomManipulation-earls court-Si(100)-H--63_1_1_cor.npy')[::-1,:][::2,::2]\n",
    "filled_cl = np.load(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\undosed\\numpy arrays\\20191122-195611_Chancery Lane-Si(001)H--22_2_0.npy')[::-1,:][::2,::2]\n",
    "empty_cl = np.load(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\undosed\\numpy arrays\\20191122-195611_Chancery Lane-Si(001)H--22_2_1_cor.npy')[::-1,:][::2,::2]\n",
    "filled_k = np.load(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\undosed\\numpy arrays\\20200731-132153_Kenton-Si(001)-H--12_1_0.npy')[::-1,:][::2,::2]\n",
    "empty_k = np.load(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\undosed\\numpy arrays\\20200731-132153_Kenton-Si(001)-H--12_1_1_cor.npy')[::-1,:][::2,::2]\n",
    "\n",
    "filled_AsEC = np.load(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\dosed\\numpy arrays\\20181026-191252_STM_AtomManipulation-Earls Court-Si(100)-H-litho-AsH3--31_1_0.npy')[::-1,:][::2,::2]\n",
    "empty_AsEC = np.load(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\dosed\\numpy arrays\\20181026-191252_STM_AtomManipulation-Earls Court-Si(100)-H-litho-AsH3--31_1_1_cor.npy')[::-1,:][::2,::2]\n",
    "filled_AsH0 = np.load(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\dosed\\numpy arrays\\20200212-183707_Hainault-Si(001)-H--54_1_0.npy')[::-1,:][::2,::2]\n",
    "empty_AsH0 = np.load(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\dosed\\numpy arrays\\20200212-183707_Hainault-Si(001)-H--54_1_0_cor.npy')[::-1,:][::2,::2]\n",
    "filled_AsH1 = np.load(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\dosed\\numpy arrays/20200219-180436_Hainault-Si(001)-H--38_1_2.npy')[::-1,:]\n",
    "empty_AsH1 =  np.load(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\dosed\\numpy arrays\\20200219-180436_Hainault-Si(001)-H--38_1_3_cor.npy')[::-1,:]\n",
    "filled_AsH2 = np.load(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\dosed\\numpy arrays/20200217-105833_Hainault-Si(001)-H-AsH3--30_2_2.npy')[::-1,:][::2,::2]\n",
    "empty_AsH2 = np.load(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\dosed\\numpy arrays\\20200217-105833_Hainault-Si(001)-H-AsH3--30_2_3_cor.npy')[::-1,:][::2,::2]\n",
    "\n",
    "#error_gr = plt.imread(r'C:\\Users\\nkolev\\Documents\\image processing\\AsH3 identification\\undosed\\error png\\20181015-142255_STM_AtomManipulation-Gloucester Road-Si(100)-H--9_1_I.png')[::2,::2,0]\n",
    "#error_ec = plt.imread(r'C:\\Users\\nkolev\\Documents\\image processing\\AsH3 identification\\undosed\\error png\\20181019-110413_STM_AtomManipulation-earls court-Si(100)-H--63_1_0I.png')[::2,::2]\n",
    "#error_cl = plt.imread(r'C:\\Users\\nkolev\\Documents\\image processing\\AsH3 identification\\undosed\\error png\\20191122-195611_Chancery Lane-Si(001)H--22_2_I.png')[::2,::2]\n",
    "#error_k = plt.imread(r'C:\\Users\\nkolev\\Documents\\image processing\\AsH3 identification\\undosed\\error png\\20200731-132153_Kenton-Si(001)-H--12_1_1_I.png')[::2,::2]\n",
    "\n",
    "#error_AsEC = plt.imread(r'error png\\20181026-191252_STM_AtomManipulation-Earls Court-Si(100)-H-litho-AsH3--31_1_I0.png')[::2,::2]\n",
    "#error_AsH0 = plt.imread(r'error png\\20200212-183707_Hainault-Si(001)-H--54_1_0I.png')\n",
    "#error_AsH1 =  plt.imread(r'error png\\20200219-180436_Hainault-Si(001)-H--38_1_I2.png')\n",
    "#error_AsH2 = plt.imread(r'error png\\20200217-105833_Hainault-Si(001)-H-AsH3--30_2_2I.png')[::2,::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b9ddff-bd85-43f3-95fb-a63d9da4d115",
   "metadata": {},
   "source": [
    "#### labels: 1 = singling dangling bonds, 2 = double dangling bond, 3= anomaly, 4 = dimer vacancy (or any dark feature really), 5 = background Silicon/Hydrogen, 6 = step edge, 7 = As feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad4b63fc-b16b-4def-978e-b4db8e4eae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the labels from the .csv files\n",
    "pd_labels_gr = pd.read_csv(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\undosed\\labels for original scans\\scans_labels_pts_20181015_gloucester_rd_9_1.csv')\n",
    "pd_labels_ec = pd.read_csv(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\undosed\\labels for original scans\\scans_labels_pts_20181019_earls_court_63_1.csv')\n",
    "pd_labels_cl = pd.read_csv(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\undosed\\labels for original scans\\scans_labels_pts_20191122_chancery_lane_22_2.csv')\n",
    "pd_labels_k = pd.read_csv(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\undosed\\labels for original scans\\scans_labels_pts_20200731_kenton_12_1.csv')\n",
    "\n",
    "pd_labels_AsEC = pd.read_csv(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\dosed\\labels for original scans\\20181026-191252_STM_AtomManipulation-Earls Court-Si(100)-H-litho-AsH3--31_10_coords.csv')\n",
    "pd_labels_AsH0 = pd.read_csv(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\dosed\\labels for original scans\\20200212-183707_Hainault-Si(001)-H--54_1_0Z_coords.csv')\n",
    "pd_labels_AsH1 = pd.read_csv(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\dosed\\labels for original scans\\20200219-180436_Hainault-Si(001)-H--38_10_coords.csv')\n",
    "pd_labels_AsH2 = pd.read_csv(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\dosed\\labels for original scans\\20200217-105833_Hainault-Si(001)-H-AsH3--30_2_2_coords.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b29aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0599df6-87c5-4df9-ba39-03909fa6dfda",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Make a separate set of data for a validation set from scans that are not in the training data at all. This will make the validation accuracy a more reliable measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d944f2f7-4e3b-4450-ae26-32ca14946147",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_cl2 = np.load(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\undosed\\numpy arrays\\20191122-195611_Chancery Lane-Si(001)H--24_6_0.npy')[::-1,:][::2,::2]\n",
    "empty_cl2 = np.load(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\undosed\\numpy arrays\\20191122-195611_Chancery Lane-Si(001)H--24_6_1_cor.npy')[::-1,:][::2,::2]\n",
    "filled_AsEC2 = np.load(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\dosed\\numpy arrays\\20181123-122007_STM_AtomManipulation-Earls Court-Si(100)-H term--26_2_0.npy')[::-1,:][::2,::2]\n",
    "empty_AsEC2 = np.load(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\dosed\\numpy arrays\\20181123-122007_STM_AtomManipulation-Earls Court-Si(100)-H term--26_2_1_cor.npy')[::-1,:][::2,::2]\n",
    "\n",
    "pd_labels_cl2 = pd.read_csv(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\undosed\\labels for original scans\\scans_labels_pts_20191122_chancery_lane_24_6.csv')\n",
    "pd_labels_AsEC2 = pd.read_csv(r'C:\\Users\\nkolev\\OneDrive - University College London\\Documents\\image processing\\AsH3 identification\\dosed\\labels for original scans\\20181123-122007_STM_AtomManipulation-Earls Court-Si(100)-H term--26_2_0_coords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05e98006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all images non-negative\n",
    "def normalise_image(image):\n",
    "    image = (image - np.min(image)) #/ (np.max(image) - np.min(image)) \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e6ba5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_gr = normalise_image(filled_gr)\n",
    "empty_gr = normalise_image(empty_gr)\n",
    "filled_ec = normalise_image(filled_ec)\n",
    "empty_ec = normalise_image(empty_ec)\n",
    "filled_cl = normalise_image(filled_cl)\n",
    "empty_cl = normalise_image(empty_cl)\n",
    "filled_k = normalise_image(filled_k)\n",
    "empty_k = normalise_image(empty_k)\n",
    "\n",
    "filled_AsEC = normalise_image(filled_AsEC)\n",
    "empty_AsEC = normalise_image(empty_AsEC)\n",
    "filled_AsH0 = normalise_image(filled_AsH0)\n",
    "empty_AsH0 = normalise_image(empty_AsH0)\n",
    "filled_AsH1 = normalise_image(filled_AsH1)\n",
    "empty_AsH1 = normalise_image(empty_AsH1)\n",
    "filled_AsH2 = normalise_image(filled_AsH2)\n",
    "empty_AsH2 = normalise_image(empty_AsH2)\n",
    "filled_cl2 = normalise_image(filled_cl2)\n",
    "empty_cl2 = normalise_image(empty_cl2)\n",
    "filled_AsEC2 = normalise_image(filled_AsEC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4c8e64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6379094859128145 0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(filled_gr), np.min(filled_gr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca15d8b4-541f-4087-ab6e-08a02477eddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# array with first column being the x-coord pixel, the second being the y coord pixel and the third being the label.\n",
    "coords_and_labels_array_gr = pd_labels_gr.values.copy() \n",
    "coords_and_labels_array_ec = pd_labels_ec.values.copy() \n",
    "coords_and_labels_array_cl = pd_labels_cl.values.copy() \n",
    "coords_and_labels_array_cl2 = pd_labels_cl2.values.copy() \n",
    "coords_and_labels_array_k = pd_labels_k.values.copy() \n",
    "\n",
    "coords_and_labels_array_AsEC = pd_labels_AsEC.values.copy() \n",
    "coords_and_labels_array_AsH0 = pd_labels_AsH0.values.copy() \n",
    "coords_and_labels_array_AsH1 = pd_labels_AsH1.values.copy() \n",
    "coords_and_labels_array_AsH2 = pd_labels_AsH2.values.copy()\n",
    "coords_and_labels_array_AsEC2 = pd_labels_AsEC2.values.copy()[:-11,:3] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d081a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the labels on the images\n",
    "def show_labels_on_image(image, coords_and_labels_array):\n",
    "    img_normalized = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    image_rgb = cv2.cvtColor(img_normalized, cv2.COLOR_GRAY2RGB)\n",
    "    for i in range(coords_and_labels_array.shape[0]):\n",
    "        x = coords_and_labels_array[i,0]\n",
    "        y = coords_and_labels_array[i,1]\n",
    "        label = coords_and_labels_array[i,2]\n",
    "        cv2.putText(image_rgb, str(int(label)), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bbb59fb-363d-481c-ac32-0534b35f531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# odd number so symmetric about middle pixel\n",
    "crop_size = 14\n",
    "pixel_shift = int(round(crop_size/2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2f0b631-e800-4373-b5f5-b67b578d1a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hainault 0 and 1 are 200nm*200nm. Need to split them up into 4 and change the coordinates labels appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eee97557-a8dd-4b11-824b-7eb64e07c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 512\n",
    "#filled_AsH01 = filled_AsH0[:512,:512].copy()\n",
    "#filled_AsH02 = filled_AsH0[:512,512:].copy()\n",
    "#filled_AsH03 = filled_AsH0[512:,:512].copy()\n",
    "#filled_AsH04 = filled_AsH0[512:,512:].copy()\n",
    "\n",
    "#error_AsH01 = error_AsH0[:512,:512].copy()\n",
    "#error_AsH02 = error_AsH0[:512,512:].copy()\n",
    "#error_AsH03 = error_AsH0[512:,:512].copy()\n",
    "#error_AsH04 = error_AsH0[512:,512:].copy()\n",
    "\n",
    "#coords_and_labels_array_AsH01 = coords_and_labels_array_AsH0[(coords_and_labels_array_AsH0[:,0]<513)&(coords_and_labels_array_AsH0[:,1]<513)].copy()\n",
    "#coords_and_labels_array_AsH02 = coords_and_labels_array_AsH0[(coords_and_labels_array_AsH0[:,1]<513)&(coords_and_labels_array_AsH0[:,0]>512)].copy()\n",
    "#coords_and_labels_array_AsH03 = coords_and_labels_array_AsH0[(coords_and_labels_array_AsH0[:,1]>512)&(coords_and_labels_array_AsH0[:,0]<513)].copy()\n",
    "#coords_and_labels_array_AsH04 = coords_and_labels_array_AsH0[(coords_and_labels_array_AsH0[:,0]>512)&(coords_and_labels_array_AsH0[:,1]>512)].copy() \n",
    "\n",
    "# need to shift these so that the top right corner of each one is its own origin\n",
    "#coords_and_labels_array_AsH02[:,0] = coords_and_labels_array_AsH02[:,0] - 513*np.ones(coords_and_labels_array_AsH02.shape[0])\n",
    "#coords_and_labels_array_AsH03[:,1] = coords_and_labels_array_AsH03[:,1] - 513*np.ones(coords_and_labels_array_AsH03.shape[0])\n",
    "#coords_and_labels_array_AsH04[:,:2] = coords_and_labels_array_AsH04[:,:2] - 513*np.ones( (coords_and_labels_array_AsH04.shape[0],2))\n",
    "\n",
    "filled_AsH11 = filled_AsH1[:512,:512].copy()\n",
    "filled_AsH12 = filled_AsH1[:512,512:].copy()\n",
    "filled_AsH13 = filled_AsH1[512:,:512].copy()\n",
    "filled_AsH14 = filled_AsH1[512:,512:].copy()\n",
    "\n",
    "empty_AsH11 = empty_AsH1[:512,:512].copy()\n",
    "empty_AsH12 = empty_AsH1[:512,512:].copy()\n",
    "empty_AsH13 = empty_AsH1[512:,:512].copy()\n",
    "empty_AsH14 = empty_AsH1[512:,512:].copy()\n",
    "\n",
    "coords_and_labels_array_AsH11 = coords_and_labels_array_AsH1[(coords_and_labels_array_AsH1[:,0]<513)&(coords_and_labels_array_AsH1[:,1]<513)].copy()\n",
    "coords_and_labels_array_AsH12 = coords_and_labels_array_AsH1[(coords_and_labels_array_AsH1[:,1]<513)&(coords_and_labels_array_AsH1[:,0]>512)].copy()\n",
    "coords_and_labels_array_AsH13 = coords_and_labels_array_AsH1[(coords_and_labels_array_AsH1[:,1]>512)&(coords_and_labels_array_AsH1[:,0]<513)].copy()\n",
    "coords_and_labels_array_AsH14 = coords_and_labels_array_AsH1[(coords_and_labels_array_AsH1[:,0]>512)&(coords_and_labels_array_AsH1[:,1]>512)].copy() \n",
    "\n",
    "# need to shift these so that the top right corner of each one is its own origin\n",
    "coords_and_labels_array_AsH12[:,0] = coords_and_labels_array_AsH12[:,0] - 513*np.ones(coords_and_labels_array_AsH12.shape[0])\n",
    "coords_and_labels_array_AsH13[:,1] = coords_and_labels_array_AsH13[:,1] - 513*np.ones(coords_and_labels_array_AsH13.shape[0])\n",
    "coords_and_labels_array_AsH14[:,:2] = coords_and_labels_array_AsH14[:,:2] - 513*np.ones( (coords_and_labels_array_AsH14.shape[0],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "185c3fde-a849-48ef-8fd8-83479824402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hainault 2 and earls court 2 are 100nm*100nm but also 1024*1024 pixels. Needs to divide the coordinates by 2\n",
    "\n",
    "coords_and_labels_array_AsH2[:,:2] = (coords_and_labels_array_AsH2[:,:2]//2).astype(np.uint64)\n",
    "coords_and_labels_array_AsEC2[:,:2] = (coords_and_labels_array_AsEC2[:,:2]//2).astype(np.uint64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f71e98c4-dd33-4127-bdfa-d262e5a39b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to recentre crop on brightest pixel \n",
    "def recentre(crop, scan, coordinate, crop_size, min_border, max_border):\n",
    "        #print(scan.shape, crop_size, crop.shape)\n",
    "        crop_shape = crop.shape\n",
    "        # make a copy of the crop that has no pixel below 0\n",
    "        crop_copy = crop-np.min(crop)\n",
    "        cropc = np.zeros(crop_shape)\n",
    "        # use crop only within the specified borders\n",
    "        cropc[min_border:max_border,min_border:max_border] = crop_copy[min_border:max_border,min_border:max_border].copy()\n",
    "        # get brightest pixel in the crop\n",
    "        brightest_pix = np.unravel_index(np.argmax(cropc), cropc.shape)\n",
    "        # The top-left of the crop is at coordinate - (crop_size - 1) for even or coordinate - crop_size for odd\n",
    "        # A simpler way is to calculate the offset from the crop center\n",
    "        crop_center_coord = np.array([d // 2 for d in crop_shape])\n",
    "        offset = np.array(brightest_pix) - crop_center_coord\n",
    "        # swap offset[0] and offset[1] to match the coordinate system\n",
    "        offset = np.array([offset[1], offset[0]])\n",
    "        new_centre = coordinate.copy() + offset\n",
    "        # label scan with new centre \n",
    "       \n",
    "        bp = brightest_pix\n",
    "        if (new_centre != coordinate).any():\n",
    "            # only redefine the crop if the centre has actually been moved\n",
    "            x, y = new_centre\n",
    "            half_size = crop_size // 2\n",
    "            if crop_size%2 == 0:\n",
    "                crop = scan[ int(y-half_size):int(y+half_size), int(x-half_size):int(x+half_size)].copy()\n",
    "                bp = np.unravel_index( np.argmax(crop-np.min(crop)), (crop_size,crop_size) )\n",
    "            else:\n",
    "                crop = scan[ int(y-half_size):int(y+half_size+1), int(x-half_size):int(x+half_size+1)].copy()\n",
    "                bp = np.unravel_index( np.argmax(crop-np.min(crop)), (crop_size,crop_size) )\n",
    "        return crop, bp, new_centre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cbe4052-3cad-4c03-8d33-bfa3c2ea5857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the crops\n",
    "def crop_scans(filled_array, empty_array, coords_and_labels_array, crop_size=crop_size, pixel_shift=pixel_shift, res=512):\n",
    "   # print(1)\n",
    "    cropped_scans = np.zeros((1,crop_size,crop_size,2))\n",
    "    coords_labels_copy = coords_and_labels_array.copy()\n",
    "    new_labels = []\n",
    "    \n",
    "    # in case of defects near the edge of the scan, we pad the array by mirroring the pixels along the edges\n",
    "    filled_mir = np.pad(filled_array, pad_width = 2*pixel_shift, mode='reflect')\n",
    "    empty_mir = np.pad(empty_array, pad_width = 2*pixel_shift, mode='reflect')\n",
    "    # because of this, we now need to shift the coordinate of the defects \n",
    "    coords_labels_copy[:,:2] += np.array([2*pixel_shift,2*pixel_shift])\n",
    "   # print(coords_labels_copy[:10,:2])\n",
    "    # loop through all the labels and take a crop of each feature\n",
    "    min_border = 5\n",
    "    max_border = crop_size - min_border\n",
    "    for i in range(coords_and_labels_array.shape[0]):\n",
    "        coord = coords_labels_copy[i,:2].copy()\n",
    "        #print(coord)\n",
    "        scan_filled = filled_mir[int(coord[1]-pixel_shift):int(coord[1]+pixel_shift), int(coord[0]-pixel_shift):int(coord[0]+pixel_shift)].copy()\n",
    "        scan_empty = empty_mir[int(coord[1]-pixel_shift):int(coord[1]+pixel_shift), int(coord[0]-pixel_shift):int(coord[0]+pixel_shift)].copy()\n",
    "        label = coords_labels_copy[i,2]\n",
    "        # input to the network will be centred on brightest spot (for bright features) so we should do that for training data too\n",
    "        # we find the brightest pixel separately for the filled and empty state scans so that we can standardise the input to the CNN\n",
    "\n",
    "        if label == 1 or label == 2 or label == 3 or label == 7:\n",
    "              # recentre filled states array\n",
    "              #if label == 7:\n",
    "              #     fig, ax = plt.subplots(1,2, figsize=(20,10))\n",
    "              #     ax[0].imshow(scan_filled, cmap='afmhot')\n",
    "              #     ax[0].set_title('filled state before recentre')\n",
    "              #     ax[1].imshow(scan_empty, cmap='afmhot')\n",
    "              #     ax[1].set_title('empty state before recentre')\n",
    "              #     plt.show() \n",
    "              scan_filled, bp1, coord_f = recentre(scan_filled, filled_mir.copy(), coord, crop_size, min_border=min_border, max_border=max_border)\n",
    "              # recentre empty states array \n",
    "              scan_empty, bp2, coord_e = recentre(scan_empty, empty_mir.copy(), coord, crop_size, min_border=min_border, max_border=max_border) \n",
    "              #if label == 7:\n",
    "              #     fig, ax = plt.subplots(1,2, figsize=(20,10))\n",
    "              #     ax[0].imshow(scan_filled, cmap='afmhot')\n",
    "              #     ax[0].set_title('filled state after recentre')\n",
    "              #     ax[1].imshow(scan_empty, cmap='afmhot')\n",
    "              #     ax[1].set_title('empty state after recentre')\n",
    "              #     plt.show()\n",
    "                   \n",
    "              scan_filled = np.expand_dims(scan_filled, axis=0)\n",
    "              scan_empty = np.expand_dims(scan_empty, axis=0)\n",
    "              scan_both = np.stack((scan_filled, scan_empty), axis=-1)\n",
    "              cropped_scans = np.concatenate((cropped_scans, scan_both), axis=0)\n",
    "              new_labels.append(label)\n",
    "        else:\n",
    "            scan_filled = np.expand_dims(scan_filled, axis=0)\n",
    "            scan_empty = np.expand_dims(scan_empty, axis=0)\n",
    "            scan_both = np.stack((scan_filled, scan_empty), axis=-1)\n",
    "            cropped_scans = np.concatenate((cropped_scans, scan_both), axis=0)\n",
    "            new_labels.append(label)\n",
    "            \n",
    "    labels = np.array(new_labels)-1\n",
    "    cropped_scans = cropped_scans[1:,:,:,:] # get rid of that initial zero array\n",
    "    return cropped_scans, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b47186f3-70a1-43af-87ed-7da0b7dd0a85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "label 0 has 251\n",
      "label 1 has 160\n",
      "label 2 has 97\n",
      "label 3 has 227\n",
      "label 4 has 237\n",
      "label 5 has 166\n",
      "label 6 has 349\n",
      "Validation set:\n",
      "label 0 has 63\n",
      "label 1 has 37\n",
      "label 2 has 27\n",
      "label 3 has 64\n",
      "label 4 has 49\n",
      "label 5 has 93\n",
      "label 6 has 22\n",
      "Test set:\n",
      "label 0 has 35\n",
      "label 1 has 19\n",
      "label 2 has 37\n",
      "label 3 has 22\n",
      "label 4 has 52\n",
      "label 5 has 0\n",
      "label 6 has 71\n"
     ]
    }
   ],
   "source": [
    "cropped_scans_ec, labels_ec = crop_scans(filled_ec, empty_ec, coords_and_labels_array_ec)\n",
    "cropped_scans_cl, labels_cl = crop_scans(filled_cl, empty_cl, coords_and_labels_array_cl)\n",
    "cropped_scans_k, labels_k =crop_scans(filled_k, empty_k, coords_and_labels_array_k)\n",
    "cropped_scans_gr, labels_gr = crop_scans(filled_gr, empty_gr, coords_and_labels_array_gr)\n",
    "cropped_scans_AsH11, labels_AsH11 =crop_scans(filled_AsH11, empty_AsH11, coords_and_labels_array_AsH11)\n",
    "cropped_scans_AsH12, labels_AsH12 =crop_scans(filled_AsH12, empty_AsH12, coords_and_labels_array_AsH12)\n",
    "cropped_scans_AsH13, labels_AsH13 =crop_scans(filled_AsH13, empty_AsH13, coords_and_labels_array_AsH13)\n",
    "cropped_scans_AsH14, labels_AsH14 =crop_scans(filled_AsH14, empty_AsH14, coords_and_labels_array_AsH14)\n",
    "cropped_scans_AsH2, labels_AsH2 =crop_scans(filled_AsH2, empty_AsH2, coords_and_labels_array_AsH2)\n",
    "cropped_scans_AsH0, labels_AsH0 =crop_scans(filled_AsH0, empty_AsH0, coords_and_labels_array_AsH0)\n",
    "\n",
    "# combine all the crops from the different scans into one numpy array\n",
    "cropped_scans = np.concatenate((cropped_scans_ec, cropped_scans_gr, cropped_scans_k,\n",
    "                                cropped_scans_AsH11, cropped_scans_AsH12, cropped_scans_AsH0,\n",
    "                                cropped_scans_AsH13, cropped_scans_AsH14, cropped_scans_AsH2), axis=0)\n",
    "labels = np.concatenate((labels_ec, labels_gr,labels_k, labels_AsH11, \n",
    "                         labels_AsH12, labels_AsH0, labels_AsH13, labels_AsH14, labels_AsH2), axis=0)\n",
    "\n",
    "print('Training set:')\n",
    "for i in range(7): print('label '+str(i)+' has ' + str(np.sum(labels == i)))\n",
    "\n",
    "# make the validation set\n",
    "\n",
    "cropped_scans_cl, labels_cl = crop_scans(filled_cl, empty_cl, coords_and_labels_array_cl)\n",
    "cropped_scans_AsEC2, labels_AsEC2 =crop_scans(filled_AsEC2, empty_AsEC2, coords_and_labels_array_AsEC2)\n",
    "\n",
    "cropped_scans_val = np.concatenate((cropped_scans_cl, cropped_scans_AsEC2), axis=0)\n",
    "labels_val = np.concatenate((labels_cl, labels_AsEC2), axis=0)#\n",
    "\n",
    "print('Validation set:')\n",
    "for i in range(7): print('label '+str(i)+' has ' + str(np.sum(labels_val == i)))\n",
    "\n",
    "# make the test set\n",
    "\n",
    "cropped_scans_AsEC, labels_AsEC =crop_scans(filled_AsEC, empty_AsEC, coords_and_labels_array_AsEC)\n",
    "cropped_scans_cl2, labels_cl2 = crop_scans(filled_cl2, empty_cl2, coords_and_labels_array_cl2)\n",
    "\n",
    "cropped_scans_test = np.concatenate((cropped_scans_AsEC, cropped_scans_cl2), axis=0)\n",
    "labels_test = np.concatenate((labels_AsEC, labels_cl2), axis=0)\n",
    "\n",
    "print('Test set:')\n",
    "for i in range(7): print('label '+str(i)+' has ' + str(np.sum(labels_test == i)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f764ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scans = [cropped_scans_ec, cropped_scans_AsH2, cropped_scans_gr, cropped_scans_k, cropped_scans_AsH11, cropped_scans_AsH12, cropped_scans_AsH13, cropped_scans_AsH14, cropped_scans_AsEC2]\n",
    "train_scan_names = ['cropped_scans_ec', 'cropped_scans_AsH2', 'cropped_scans_gr', 'cropped_scans_k', 'cropped_scans_AsH11', 'cropped_scans_AsH12', 'cropped_scans_AsH13', 'cropped_scans_AsH14', 'cropped_scans_AsEC2']\n",
    "train_labels = [labels_ec, labels_AsH2, labels_gr, labels_k, labels_AsH11, labels_AsH12, labels_AsH13, labels_AsH14, labels_AsEC2]\n",
    "val_scans = [cropped_scans_cl, cropped_scans_AsH0]\n",
    "val_scan_names = ['cropped_scans_cl', 'cropped_scans_AsH0']\n",
    "val_labels = [labels_cl, labels_AsH0]\n",
    "test_scans = [cropped_scans_cl2, cropped_scans_AsEC]\n",
    "test_scan_names = ['cropped_scans_cl2', 'cropped_scans_AsEC']\n",
    "test_labels = [labels_cl2, labels_AsEC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "420d03c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Label 0 statistics:\n",
      "--------------------------------------------------\n",
      "Train set\n",
      "cropped_scans_ec label 0 has 9\n",
      "cropped_scans_AsH2 label 0 has 0\n",
      "cropped_scans_gr label 0 has 84\n",
      "cropped_scans_k label 0 has 158\n",
      "cropped_scans_AsH11 label 0 has 0\n",
      "cropped_scans_AsH12 label 0 has 0\n",
      "cropped_scans_AsH13 label 0 has 0\n",
      "cropped_scans_AsH14 label 0 has 0\n",
      "cropped_scans_AsEC2 label 0 has 0\n",
      "Total number of labels in training set: 251\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Validation set\n",
      "cropped_scans_cl label 0 has 63\n",
      "cropped_scans_AsH0 label 0 has 0\n",
      "Total number of labels in validation set: 63\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Test set\n",
      "cropped_scans_cl2 label 0 has 35\n",
      "cropped_scans_AsEC label 0 has 0\n",
      "Total number of labels in test set: 35\n",
      "Percentage of labels in training set: 71.91977077363897%\n",
      "Percentage of labels in validation set: 18.05157593123209%\n",
      "Percentage of labels in test set: 10.028653295128938%\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Label 1 statistics:\n",
      "--------------------------------------------------\n",
      "Train set\n",
      "cropped_scans_ec label 1 has 1\n",
      "cropped_scans_AsH2 label 1 has 0\n",
      "cropped_scans_gr label 1 has 35\n",
      "cropped_scans_k label 1 has 124\n",
      "cropped_scans_AsH11 label 1 has 0\n",
      "cropped_scans_AsH12 label 1 has 0\n",
      "cropped_scans_AsH13 label 1 has 0\n",
      "cropped_scans_AsH14 label 1 has 0\n",
      "cropped_scans_AsEC2 label 1 has 0\n",
      "Total number of labels in training set: 160\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Validation set\n",
      "cropped_scans_cl label 1 has 37\n",
      "cropped_scans_AsH0 label 1 has 0\n",
      "Total number of labels in validation set: 37\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Test set\n",
      "cropped_scans_cl2 label 1 has 19\n",
      "cropped_scans_AsEC label 1 has 0\n",
      "Total number of labels in test set: 19\n",
      "Percentage of labels in training set: 74.07407407407408%\n",
      "Percentage of labels in validation set: 17.12962962962963%\n",
      "Percentage of labels in test set: 8.796296296296296%\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Label 2 statistics:\n",
      "--------------------------------------------------\n",
      "Train set\n",
      "cropped_scans_ec label 2 has 0\n",
      "cropped_scans_AsH2 label 2 has 60\n",
      "cropped_scans_gr label 2 has 17\n",
      "cropped_scans_k label 2 has 12\n",
      "cropped_scans_AsH11 label 2 has 0\n",
      "cropped_scans_AsH12 label 2 has 0\n",
      "cropped_scans_AsH13 label 2 has 0\n",
      "cropped_scans_AsH14 label 2 has 0\n",
      "cropped_scans_AsEC2 label 2 has 21\n",
      "Total number of labels in training set: 110\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Validation set\n",
      "cropped_scans_cl label 2 has 6\n",
      "cropped_scans_AsH0 label 2 has 8\n",
      "Total number of labels in validation set: 14\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Test set\n",
      "cropped_scans_cl2 label 2 has 27\n",
      "cropped_scans_AsEC label 2 has 10\n",
      "Total number of labels in test set: 37\n",
      "Percentage of labels in training set: 68.32298136645963%\n",
      "Percentage of labels in validation set: 8.695652173913043%\n",
      "Percentage of labels in test set: 22.981366459627328%\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Label 6 statistics:\n",
      "--------------------------------------------------\n",
      "Train set\n",
      "cropped_scans_ec label 6 has 0\n",
      "cropped_scans_AsH2 label 6 has 88\n",
      "cropped_scans_gr label 6 has 0\n",
      "cropped_scans_k label 6 has 0\n",
      "cropped_scans_AsH11 label 6 has 44\n",
      "cropped_scans_AsH12 label 6 has 32\n",
      "cropped_scans_AsH13 label 6 has 56\n",
      "cropped_scans_AsH14 label 6 has 48\n",
      "cropped_scans_AsEC2 label 6 has 22\n",
      "Total number of labels in training set: 290\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Validation set\n",
      "cropped_scans_cl label 6 has 0\n",
      "cropped_scans_AsH0 label 6 has 81\n",
      "Total number of labels in validation set: 81\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Test set\n",
      "cropped_scans_cl2 label 6 has 0\n",
      "cropped_scans_AsEC label 6 has 71\n",
      "Total number of labels in test set: 71\n",
      "Percentage of labels in training set: 65.61085972850678%\n",
      "Percentage of labels in validation set: 18.32579185520362%\n",
      "Percentage of labels in test set: 16.063348416289593%\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print the number of labels in each image\n",
    "\n",
    "\n",
    "for j in [0,1,2,6]:\n",
    "    print('-'*50)\n",
    "    print('Label ' + str(j) + ' statistics:')\n",
    "    print('-'*50)\n",
    "    print('Train set')\n",
    "    total_train = 0\n",
    "    for i, scan in enumerate(train_scans):\n",
    "        #for j in range(7):\n",
    "            print(train_scan_names[i] + ' label ' + str(j) + ' has ' + str(np.sum(train_labels[i] == j)))\n",
    "            total_train += np.sum(train_labels[i] == j)\n",
    "    print('Total number of labels in training set: ' + str(total_train))\n",
    "\n",
    "    print('-'*50)\n",
    "    print('-'*50)\n",
    "    print('Validation set')\n",
    "    total_val = 0\n",
    "    for i, scan in enumerate(val_scans):\n",
    "        #for j in range(7):\n",
    "            print(val_scan_names[i] + ' label ' + str(j) + ' has ' + str(np.sum(val_labels[i] == j)))   \n",
    "            total_val += np.sum(val_labels[i] == j)\n",
    "    print('Total number of labels in validation set: ' + str(total_val))\n",
    "\n",
    "    print('-'*50)\n",
    "    print('-'*50)\n",
    "    print('Test set')\n",
    "    total_test = 0\n",
    "    for i, scan in enumerate(test_scans):\n",
    "        #for j in range(7):\n",
    "            print(test_scan_names[i] + ' label ' + str(j) + ' has ' + str(np.sum(test_labels[i] == j)))\n",
    "            total_test += np.sum(test_labels[i] == j)\n",
    "    print('Total number of labels in test set: ' + str(total_test))\n",
    "\n",
    "    print('Percentage of labels in training set: ' + str(total_train/(total_train+total_val+total_test)*100) + '%')\n",
    "    print('Percentage of labels in validation set: ' + str(total_val/(total_train+total_val+total_test)*100) + '%')\n",
    "    print('Percentage of labels in test set: ' + str(total_test/(total_train+total_val+total_test)*100) + '%')\n",
    "    print('-'*50)\n",
    "    print('-'*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99aec533-9d80-444c-80e4-b8a5c0ff67e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1487, 14, 14, 2), (1487,), (355, 14, 14, 2), (355,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cropped_scans.shape, labels.shape, cropped_scans_val.shape, labels_val.shape # check how much data I have\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0b7a8-fb4e-42d5-b651-a08263545dfd",
   "metadata": {},
   "source": [
    "Now do some data augmentation. I rotate each data point by 90, 180, and 270 degrees and also take the mirror of each data point and its rotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79decdbc-d44e-4021-84de-8d85d645db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rot_flip(crops, labels):\n",
    "    copy_of_crop = np.copy(crops)\n",
    "    flipped = np.fliplr(copy_of_crop)\n",
    "    crops = np.concatenate((crops, flipped), axis=0)\n",
    "\n",
    "    for i in range(3):\n",
    "        rot = np.rot90(copy_of_crop, k=(i+1), axes=(1,2) )\n",
    "        crops = np.concatenate((crops, rot), axis=0)\n",
    "        flipped = np.fliplr(rot)\n",
    "        crops = np.concatenate((crops, flipped), axis=0)\n",
    "    \n",
    "    labels = np.tile(labels, 8) # now need to extend the label array too but everythings in the same order so can just repeat it 8 times.\n",
    "    #crops=np.transpose(crops, (3,0,1,2))\n",
    "    return crops, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4349905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, a,b):\n",
    "    '''\n",
    "    A sigmoid function that takes in an array x and parameters a and b.\n",
    "    This is used to rescale the pixel values in the \"doubled\" image (the brighter ones\n",
    "    are often the ones that are doubled more clearly).\n",
    "    args:\n",
    "    x: array of pixel values\n",
    "    a: parameter of the sigmoid function\n",
    "    b: parameter of the sigmoid function\n",
    "    returns:\n",
    "    array of pixel values rescaled by the sigmoid function\n",
    "    '''\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "    output = 1 / (1 + np.exp(a-b*x))\n",
    "    output = np.array(output)\n",
    "    return output\n",
    "\n",
    "def add_blur_and_noise(crops, labels, do_double_tip = True, min_sigma=0.1, max_sigma=0.5, gauss_kern_size = 5, min_noise_level = 0.1, max_noise_level=0.5, plot=False):\n",
    "    blurred_noisy_crops = []    \n",
    "\n",
    "    i = 0\n",
    "    for crop in crops:\n",
    "        N = crop_size * 10\n",
    "        # first upsample to (N,N)\n",
    "        crop_rs = cv2.resize(crop, (N, N), interpolation=cv2.INTER_LINEAR)\n",
    "        # choose a random sigma for Gaussian blur\n",
    "        sigma = np.random.uniform(min_sigma, max_sigma)  \n",
    "        # choose a random noise level\n",
    "        noise_level = np.random.uniform(min_noise_level, max_noise_level)\n",
    "       # print('noise level: ', noise_level)\n",
    "        # Apply Gaussian blur\n",
    "        blurred_crop = cv2.GaussianBlur(crop_rs, (gauss_kern_size, gauss_kern_size), sigma)\n",
    "        # Add Gaussian noise\n",
    "        noise = np.random.normal(0, noise_level, crop_rs.shape)\n",
    "        blurred_noisy_crop = blurred_crop + noise \n",
    "        # now add a slight double tip with 50% probability\n",
    "        if do_double_tip:\n",
    "            double_tip = np.random.rand() < 0.5\n",
    "            if double_tip:\n",
    "                b_min, b_max = 8, 10\n",
    "                a_min, a_max = 7, 9\n",
    "                multiple_min, multiple_max = 1, 2.5\n",
    "                # get random values for a and b\n",
    "                b = np.random.uniform(b_min, b_max)\n",
    "                a = np.random.uniform(a_min, a_max)\n",
    "                multiple = np.random.uniform(multiple_min, multiple_max)        \n",
    "                # Apply the sigmoid function to each pixel value\n",
    "                sigmoid_array = multiple*sigmoid(blurred_noisy_crop,a,b)\n",
    "                # cap sigmoid_array to 1\n",
    "                sigmoid_array = (sigmoid_array - np.min(sigmoid_array))/(np.max(sigmoid_array)-np.min(sigmoid_array))\n",
    "                # Offset each pixel by a random value in x and y (between 5 and 10)\n",
    "                # Generate random offsets for x and y\n",
    "                offset_x = torch.randint(5, 15, (1,))[0] \n",
    "                offset_y = torch.randint(5, 15, (1,))[0]\n",
    "                # make negative with 50% chance\n",
    "                if random.random() < 0.5:\n",
    "                    offset_x = -offset_x\n",
    "                if random.random() < 0.5:\n",
    "                    offset_y = -offset_y\n",
    "                # apply a random nxn kernel\n",
    "                # n is random integer between 2 and 8\n",
    "                n = np.random.randint(30, 50)\n",
    "                kernel = np.random.uniform(-0.5, 1, (1,n,n))\n",
    "                kernel = kernel/np.sum(kernel)\n",
    "                #print('kernel size: ', kernel.shape)\n",
    "                sigmoid_array_filled = torch.tensor(sigmoid_array[:,:,0], dtype=torch.float64).unsqueeze(0)\n",
    "                sigmoid_array_empty = torch.tensor(sigmoid_array[:,:,1], dtype=torch.float64).unsqueeze(0)\n",
    "                filtered_offset_filled = torch.nn.functional.conv2d(sigmoid_array_filled, torch.tensor(kernel).unsqueeze(0), padding='same')\n",
    "                filtered_offset_filled = filtered_offset_filled.squeeze(0)\n",
    "                filtered_offset_filled = filtered_offset_filled.numpy() \n",
    "                filtered_offset_empty = torch.nn.functional.conv2d(sigmoid_array_empty, torch.tensor(kernel).unsqueeze(0), padding='same')\n",
    "                filtered_offset_empty = filtered_offset_empty.squeeze(0)\n",
    "                filtered_offset_empty = filtered_offset_empty.numpy()\n",
    "                # stack the two arrays together\n",
    "                filtered_offset = np.stack((filtered_offset_filled, filtered_offset_empty), axis=-1)       \n",
    "                # Add the add_array to the base_array with the offset\n",
    "                # then crop it so it only includes the parts where the two arrays are overlaid\n",
    "                # Create a copy of the base array\n",
    "                result = np.copy(blurred_noisy_crop)\n",
    "                rows, cols,_ = blurred_noisy_crop.shape\n",
    "                if offset_x >= 0 and offset_y >= 0:\n",
    "                    # Positive offsets\n",
    "                    result[offset_x:, offset_y:,:] += filtered_offset[:rows-offset_x, :cols-offset_y,:]\n",
    "                elif offset_x >= 0 and offset_y < 0:\n",
    "                    # Negative y-offset\n",
    "                    result[offset_x:, :offset_y,:] += filtered_offset[:rows-offset_x, -offset_y:,:]\n",
    "                elif offset_x < 0 and offset_y >= 0:\n",
    "                    # Negative x-offset\n",
    "                    result[:offset_x, offset_y:,:] += filtered_offset[-offset_x:, :cols-offset_y,:]\n",
    "                else:  # offset_x < 0 and offset_y < 0\n",
    "                    # Negative offsets for both x and y\n",
    "                    result[:offset_x, :offset_y,:] += filtered_offset[-offset_x:, -offset_y:,:]\n",
    "            \n",
    "            # downsample back to (crop_size,crop_size)\n",
    "            if double_tip:\n",
    "                blurred_noisy_crop_rs = cv2.resize(result, (crop_size, crop_size), interpolation=cv2.INTER_LINEAR)\n",
    "            else:    \n",
    "                blurred_noisy_crop_rs = cv2.resize(blurred_noisy_crop, (crop_size, crop_size), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        else:    \n",
    "            blurred_noisy_crop_rs = cv2.resize(blurred_noisy_crop, (crop_size, crop_size), interpolation=cv2.INTER_LINEAR)\n",
    "            \n",
    "        blurred_noisy_crop_rs = blurred_noisy_crop_rs[1:crop_size-1, 1:crop_size-1]\n",
    "    \n",
    "        # Append to the list  \n",
    "        if plot:\n",
    "            if 200 < i and i < 220:\n",
    "                print('Double tip: ', double_tip)    \n",
    "                print('offset_x: ', offset_x, ' offset_y: ', offset_y)\n",
    "                print('n: ', n)\n",
    "                label = labels[i]\n",
    "                if label == 6 or label == 0 or label == 1 or label == 2:\n",
    "                    print('sigma: ', sigma) \n",
    "                    print('noise level: ', noise_level)\n",
    "                    # plot everything together\n",
    "                    fig, axs = plt.subplots(2, 3, figsize=(10, 10))\n",
    "                    axs[0,0].imshow(crop[1:crop_size-1,1:crop_size-1,0], cmap='afmhot')\n",
    "                    axs[0,0].set_title('Original Crop')\n",
    "                    axs[0,1].imshow(crop_rs[:,:,0], cmap='afmhot')\n",
    "                    axs[0,1].set_title('Upsampled Crop')\n",
    "                    axs[0,2].imshow(filtered_offset_filled, cmap='afmhot')\n",
    "                    axs[0,2].set_title('Filtered Offset Filled')\n",
    "                    axs[1,0].imshow(blurred_noisy_crop[:,:,0], cmap='afmhot')\n",
    "                    axs[1,0].set_title('Blurred and noisy Crop')\n",
    "                    axs[1,1].imshow(blurred_noisy_crop_rs[:,:,0], cmap='afmhot')\n",
    "                    axs[1,1].set_title('Blurred and Noisy Crop downsampled')\n",
    "                    axs[1,2].imshow(crop[1:crop_size-1,1:crop_size-1,0] - blurred_noisy_crop_rs[:,:,0], cmap='afmhot')\n",
    "                    axs[1,2].set_title('Difference')\n",
    "                    plt.show()\n",
    "                # print the old and new means + variances\n",
    "                print('Old mean: ', np.mean(crop[1:crop_size-1,1:crop_size-1,0]), ' Old variance: ', np.var(crop[1:crop_size-1,1:crop_size-1,0]))\n",
    "                print('New mean: ', np.mean(blurred_noisy_crop_rs[:,:,0]), ' New variance: ', np.var(blurred_noisy_crop_rs[:,:,0]))\n",
    "        blurred_noisy_crops.append(blurred_noisy_crop_rs)\n",
    "        if len(blurred_noisy_crops) % 1000 == 0:\n",
    "            print(f'Processed {len(blurred_noisy_crops)} crops with blur and noise.')\n",
    "        i += 1\n",
    "\n",
    "    blurred_noisy_crops = np.array(blurred_noisy_crops)\n",
    "\n",
    "    # Combine original, blurred, and noisy crops\n",
    "    combined_crops = np.concatenate((crops[:,1:crop_size-1,1:crop_size-1,:], blurred_noisy_crops), axis=0)\n",
    "    combined_labels = np.tile(labels, 2)  # Repeat labels for the new crops\n",
    "    \n",
    "    return combined_crops, combined_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70f66405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1487, 14, 14, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cropped_scans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e065156-d09e-4dbf-be28-1a908f79c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = rot_flip(cropped_scans, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37c8b9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nkolev\\AppData\\Local\\Temp\\ipykernel_13408\\2220995162.py:69: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Convolution.cpp:1032.)\n",
      "  filtered_offset_filled = torch.nn.functional.conv2d(sigmoid_array_filled, torch.tensor(kernel).unsqueeze(0), padding='same')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 crops with blur and noise.\n",
      "Processed 2000 crops with blur and noise.\n",
      "Processed 3000 crops with blur and noise.\n",
      "Processed 4000 crops with blur and noise.\n",
      "Processed 5000 crops with blur and noise.\n",
      "Processed 6000 crops with blur and noise.\n",
      "Processed 7000 crops with blur and noise.\n",
      "Processed 8000 crops with blur and noise.\n",
      "Processed 9000 crops with blur and noise.\n",
      "Processed 10000 crops with blur and noise.\n",
      "Processed 11000 crops with blur and noise.\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = add_blur_and_noise(train_data, train_labels, min_sigma=10, max_sigma=100, gauss_kern_size = 21, min_noise_level=0.001, max_noise_level=0.003,plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd691d8c-7d95-4a41-a8de-1bbca961343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_scans_val, labels_val = rot_flip(cropped_scans_val, labels_val)\n",
    "cropped_scans_test, labels_test = rot_flip(cropped_scans_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "415cc26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 crops with blur and noise.\n",
      "Processed 2000 crops with blur and noise.\n"
     ]
    }
   ],
   "source": [
    "cropped_scans_val, labels_val = add_blur_and_noise(cropped_scans_val, labels_val, do_double_tip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "id": "7a74e4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cropped_scans_, labels_ = add_blur_and_noise(cropped_scans, labels, min_sigma=10, max_sigma=100, gauss_kern_size = 31, min_noise_level=0.003, max_noise_level=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "977fb2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_scans_test = cropped_scans_test[:,1:crop_size-1,1:crop_size-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef89644c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5680, 12, 12, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cropped_scans_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd75c1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set after augmentation:\n",
      "label 0 has 251\n",
      "label 1 has 160\n",
      "label 2 has 97\n",
      "label 3 has 227\n",
      "label 4 has 237\n",
      "label 5 has 166\n",
      "label 6 has 349\n",
      "Validation set after augmentation:\n",
      "label 0 has 1008\n",
      "label 1 has 592\n",
      "label 2 has 432\n",
      "label 3 has 1024\n",
      "label 4 has 784\n",
      "label 5 has 1488\n",
      "label 6 has 352\n",
      "Test set after augmentation:\n",
      "label 0 has 280\n",
      "label 1 has 152\n",
      "label 2 has 296\n",
      "label 3 has 176\n",
      "label 4 has 416\n",
      "label 5 has 0\n",
      "label 6 has 568\n"
     ]
    }
   ],
   "source": [
    "print('Training set after augmentation:')\n",
    "for i in range(7): print('label '+str(i)+' has ' + str(np.sum(labels == i)))\n",
    "print('Validation set after augmentation:')\n",
    "for i in range(7): print('label '+str(i)+' has ' + str(np.sum(labels_val == i)))\n",
    "print('Test set after augmentation:')\n",
    "for i in range(7): print('label '+str(i)+' has ' + str(np.sum(labels_test == i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45c453c7-3a4a-4168-a075-34e4046ae74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1487,), (1487, 14, 14, 2), (5680,), (5680, 12, 12, 2))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape, cropped_scans.shape, labels_val.shape, cropped_scans_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98d40ea1-b7a2-4e50-92a8-eb3a615ed584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose so shape is (number of data points, num of channels, res,res)\n",
    "#cropped_scans = np.transpose(cropped_scans, axes=[0,3,1,2])\n",
    "train_data = np.transpose(train_data, axes=[0,3,1,2])\n",
    "cropped_scans_val = np.transpose(cropped_scans_val, axes=[0,3,1,2])\n",
    "cropped_scans_test = np.transpose(cropped_scans_test, axes=[0,3,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53f0190e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23792, 2, 12, 12)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c64c517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5680, 2, 12, 12)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cropped_scans_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2eeb8e-5117-48e3-8ebb-42a21a7e9243",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('DB_train.npy', cropped_scans)\n",
    "np.save('DB_train_labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6dc794-92be-42d0-a00c-5291110de475",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('DB_val.npy', cropped_scans_val)\n",
    "np.save('DB_val_labels.npy', labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7faabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('DB_test.npy', cropped_scans_test)\n",
    "np.save('DB_test_labels.npy', labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901eaa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cropped_scans = np.load('As_train.npy')\n",
    "#labels = np.load('As_train_labels.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "7eb0cf2d-f789-4a93-8b5e-9c036411f8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23792, 2, 11, 11)"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cropped_scans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "1f153e63-0398-40cc-9ee8-1b6e871c2889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5584"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(labels==6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af7b9123-7b96-450a-a2a0-be44755514ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "489f15a4-01b9-4cc7-8adb-62931e5d07bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, features,length=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.features = features\n",
    "        self.refine()\n",
    "        self.length = length if length is not None else len(self.images)\n",
    "\n",
    "\n",
    "    def refine(self):\n",
    "        # gets rid of unwanted features in the dataset and fixes labels correspondingly\n",
    "        crops = [self.images[self.labels==i,:,:,:] for i in self.features]\n",
    "        self.images = torch.vstack(crops)\n",
    "        \n",
    "        labels = [self.labels[self.labels==i] for i in self.features]\n",
    "        new_labels = []\n",
    "        \n",
    "        for i, array in enumerate(labels):\n",
    "            new_labels.append(i*np.ones(array.shape))\n",
    "        self.labels = torch.tensor(np.hstack(new_labels)).long()\n",
    "        print('Dataset refined. Number of images: ', len(self.images), ' Number of labels: ', len(self.labels))\n",
    "        \n",
    "        return \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx % self.length  # Ensure idx is within bounds\n",
    "        label = torch.clone(self.labels[idx])\n",
    "        image = torch.clone(self.images[idx])\n",
    "       # print('before: ', torch.mean(image[0,:,:]), torch.var(image[0,:,:]) , torch.mean(image[1,:,:]))\n",
    "        image[0,:,:] = (image[0,:,:]-torch.mean(image[0,:,:]))#/torch.std(image[0,:,:])\n",
    "        image[1,:,:] = (image[1,:,:]-torch.mean(image[1,:,:]))#/torch.std(image[1,:,:])\n",
    "       # print('after: ', torch.mean(image[0,:,:]), torch.var(image[0,:,:]) , torch.mean(image[1,:,:]))\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a55564e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5680, 2, 12, 12)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cropped_scans_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb5c0e79-8be6-4204-a3c1-5985473d558b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset refined. Number of images:  8128  Number of labels:  8128\n",
      "Dataset refined. Number of images:  2032  Number of labels:  2032\n",
      "Dataset refined. Number of images:  728  Number of labels:  728\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(torch.tensor(np.copy(train_data)).float(), torch.tensor(train_labels).long(), [0,1,2])\n",
    "val_data = CustomDataset(torch.tensor(np.copy(cropped_scans_val)).float(), torch.tensor(labels_val).long(), [0,1,2])\n",
    "test_data = CustomDataset(torch.tensor(np.copy(cropped_scans_test)).float(), torch.tensor(labels_test).long(), [0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c3cde83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting examples from train_data\n",
      "--- Showing examples for new label: 0 ---\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Showing examples for new label: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Find the indices of images with the current label\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(train_data\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m==\u001b[39m label_idx)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Check if there are any images with this label\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'labels'"
     ]
    }
   ],
   "source": [
    "# Number of examples to show per label\n",
    "n_examples = 5\n",
    "\n",
    "print(\"Plotting examples from train_data\")\n",
    "\n",
    "# The labels in train_data are 0, 1, 2, 3, corresponding to original labels 0, 1, 2, 6\n",
    "for label_idx in range(4):\n",
    "    print(f\"--- Showing examples for new label: {label_idx} ---\")\n",
    "    \n",
    "    # Find the indices of images with the current label\n",
    "    indices = np.where(train_data.labels.numpy() == label_idx)[0]\n",
    "    \n",
    "    # Check if there are any images with this label\n",
    "    if len(indices) == 0:\n",
    "        print(f\"No examples found for label {label_idx}\")\n",
    "        continue\n",
    "\n",
    "    # Take the first n_examples, or fewer if not enough are available\n",
    "    num_to_show = min(n_examples, len(indices))\n",
    "    \n",
    "    for i in range(num_to_show):\n",
    "        # Get an image and its label from the dataset\n",
    "        image, label = train_data[indices[i+500]]\n",
    "        \n",
    "        # Create a figure with two subplots side-by-side\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        \n",
    "        # Display the first channel (filled state)\n",
    "        ax[0].imshow(image[0], cmap='afmhot')\n",
    "        ax[0].set_title('Filled State')\n",
    "        ax[0].axis('off')\n",
    "\n",
    "        # Display the second channel (empty state)\n",
    "        ax[1].imshow(image[1], cmap='afmhot')\n",
    "        ax[1].set_title('Empty State')\n",
    "        ax[1].axis('off')\n",
    "\n",
    "        # Set a title for the entire figure\n",
    "        plt.suptitle(f'Label: {label.item()}, Example: {i+1}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "223a0085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data labels after dataset creation:\n",
      "label 0 has 4016\n",
      "label 1 has 2560\n",
      "label 2 has 1552\n",
      "label 3 has 0\n",
      "label 4 has 0\n",
      "label 5 has 0\n",
      "label 6 has 0\n",
      "Validation data labels after dataset creation:\n",
      "label 0 has 1008\n",
      "label 1 has 592\n",
      "label 2 has 432\n",
      "label 3 has 0\n",
      "label 4 has 0\n",
      "label 5 has 0\n",
      "label 6 has 0\n",
      "Test data labels after dataset creation:\n",
      "label 0 has 280\n",
      "label 1 has 152\n",
      "label 2 has 296\n",
      "label 3 has 0\n",
      "label 4 has 0\n",
      "label 5 has 0\n",
      "label 6 has 0\n"
     ]
    }
   ],
   "source": [
    "print('Train data labels after dataset creation:')\n",
    "for i in range(7): print('label '+str(i)+' has ' + str(len(np.where(train_dataset.labels == i)[0])))\n",
    "print('Validation data labels after dataset creation:')\n",
    "for i in range(7): print('label '+str(i)+' has ' + str(len(np.where(val_data.labels == i)[0])))\n",
    "print('Test data labels after dataset creation:')\n",
    "for i in range(7): print('label '+str(i)+' has ' + str(len(np.where(test_data.labels == i)[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1090,
   "id": "4262b5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23792, 2, 20, 20)"
      ]
     },
     "execution_count": 1090,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cropped_scans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cc2e52bb-7a20-4de0-9b65-cddd77c18f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4016 2560 1552]\n"
     ]
    }
   ],
   "source": [
    "# Compute the sample weights\n",
    "class_sample_count = np.array([len(np.where(train_dataset.labels== t)[0]) for t in range(3)])\n",
    "print(class_sample_count)\n",
    "weight = 1.0 / class_sample_count\n",
    "samples_weight = np.array(weight)/np.sum(weight)\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "# Create the sampler and data loader\n",
    "data_loader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "data_loader_val = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "data_loader_test = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "#ec_data_loader = DataLoader(ec_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf23eace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-1.4487e-08,  1.6350e-08]), tensor([-2.3180e-08, -4.1806e-08]), tensor([-2.4318e-08,  8.3819e-09]), tensor([-8.0715e-09, -1.0141e-08]), tensor([ 4.1392e-09, -4.1392e-10]), tensor([3.5183e-09, 3.8288e-08]), tensor([-1.5936e-08, -4.9671e-09]), tensor([6.9332e-09, 1.7126e-08]), tensor([ 1.8109e-08, -7.5541e-09]), tensor([2.1938e-08, 2.5663e-08])]\n",
      "[tensor([0.0177, 0.0160]), tensor([0.0171, 0.0665]), tensor([0.0030, 0.0031]), tensor([0.0006, 0.0012]), tensor([0.0007, 0.0045]), tensor([0.0142, 0.0678]), tensor([0.0016, 0.0055]), tensor([0.0028, 0.0027]), tensor([0.0622, 0.0208]), tensor([0.0015, 0.0011])]\n",
      "tensor([-3.1355e-09,  4.0926e-09])\n",
      "Channel-wise Mean: tensor([-3.1355e-09,  4.0926e-09])\n",
      "Channel-wise Variance: tensor([0.0121, 0.0189])\n"
     ]
    }
   ],
   "source": [
    "def calculate_mean_variance(dataloader, n):\n",
    "    samples_loaded = 0\n",
    "    channel_means = []\n",
    "    channel_variances = []\n",
    "\n",
    "    for crops, _ in dataloader:\n",
    "        for crop in crops:\n",
    "            if samples_loaded >= n:\n",
    "                break\n",
    "            channel_means.append(torch.mean(crop, dim=(1, 2)))\n",
    "            channel_variances.append(torch.var(crop, dim=(1, 2)))\n",
    "            samples_loaded += 1\n",
    "        if samples_loaded >= n:\n",
    "            break\n",
    "    print(channel_means)\n",
    "    print(channel_variances)\n",
    "    channel_means = torch.stack(channel_means).mean(dim=0)\n",
    "    channel_variances = torch.stack(channel_variances).mean(dim=0)\n",
    "    print(channel_means)\n",
    "    print(\"Channel-wise Mean:\", channel_means)\n",
    "    print(\"Channel-wise Variance:\", channel_variances)\n",
    "    return channel_means, channel_variances\n",
    "\n",
    "# Example usage\n",
    "n_samples = 10\n",
    "channel_means, channel_variances = calculate_mean_variance(data_loader_train, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a81d7a54-0572-4283-a2d6-4cf0453de9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4016, 2560, 1552]), array([0.19393455, 0.30423482, 0.50183063]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_sample_count, samples_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "554a430e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19393455, 0.30423482, 0.50183063])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f3f4d462-8faf-47cf-87e1-fa9ae153f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, channels, crop_size, n_outputs, fc_layers, fc_nodes, dropout):\n",
    "        super().__init__()\n",
    "        self.fc_layers = fc_layers\n",
    "        self.convolutional_relu_stack = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 3, stride=1, padding='valid'),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding='valid'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding='valid'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),            \n",
    "            nn.Conv2d(128, 256, 3, stride=1, padding='valid'),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(out_features=fc_nodes),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(fc_nodes),\n",
    "        )\n",
    "        \n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(fc_nodes, fc_nodes),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(fc_nodes)\n",
    "        )\n",
    "        \n",
    "        self.linear_relu_stack_last = nn.Sequential(\n",
    "            nn.Linear(fc_nodes, n_outputs)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x, training = True):\n",
    "        x = self.convolutional_relu_stack(x)\n",
    "        for i in range(self.fc_layers-1):\n",
    "            x = self.linear_relu_stack(x)\n",
    "        logits = self.linear_relu_stack_last(x) \n",
    "        if training == True:\n",
    "            return logits\n",
    "        else: \n",
    "            logits = torch.nn.functional.softmax(logits, dim=1)\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "be225d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "382a9458-c86d-4f28-821a-3f595e030ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(channels=2, crop_size=11, n_outputs=4, fc_layers=2, fc_nodes=100, dropout=0.2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2992c9b3-3838-4ae1-8117-40e9570862e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the model\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30554d72-a33c-4146-b790-d7c0d42435e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy function\n",
    "def testAccuracy(model, dataloader, device='cuda'):\n",
    "\n",
    "    model.eval()\n",
    "    #accuracy = 0.0\n",
    "    #total = 0.0\n",
    "    predictions = []\n",
    "    all_labels = [] \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            crops, labels = data\n",
    "           # total += labels.size(0)\n",
    "            crops, labels = crops.to(device), labels.to(device)\n",
    "            # run the model on the test set to predict labels\n",
    "            outputs = model(crops.float())\n",
    "            \n",
    "            # the label with the highest prob will be our prediction\n",
    "            predicted = torch.argmax(outputs.data, 1)\n",
    "            #print(predicted)\n",
    "            predictions.append(predicted.unsqueeze(1))\n",
    "            all_labels.append(labels.unsqueeze(1))\n",
    "\n",
    "            #accuracy += (predicted == labels).sum().item()\n",
    "    # stack predictions and labels into 1 array each          \n",
    "    predictions = torch.vstack(predictions)\n",
    "    all_labels = torch.vstack(all_labels)\n",
    "    conf = confusion_matrix(all_labels.cpu(), predictions.cpu())\n",
    "    \n",
    "    print(conf)    \n",
    "\n",
    "    accuracy = 100 * np.sum(np.diag(conf)) / np.sum(conf)\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b8b865-9a4b-41e7-a5bf-a16f0b560d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader_train, dataloader_test, loss_, num_epochs, path, optimizer, patience = 25, scheduler=None, device='cuda'):\n",
    "   \n",
    "    best_loss = np.inf\n",
    "    best_accuracy = 0\n",
    "    patience_counter = 0 \n",
    "    # Iterate over the training data\n",
    "    for epoch in range(num_epochs):\n",
    "        running_train_loss = 0.0\n",
    "        running_test_loss = 0.0\n",
    "\n",
    "        # train the model\n",
    "        model.train()\n",
    "        for i, (crops, labels) in enumerate(dataloader_train):\n",
    "            # Get the crops and labels\n",
    "            crops, labels = crops.to(device), labels.to(device)\n",
    "           \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # get prediction\n",
    "            outputs = model(crops.float()).float().to(device)\n",
    "            \n",
    "            loss = loss_(outputs, labels.long())\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        print('Confusion matrix from train data')\n",
    "        accuracy = testAccuracy(model, dataloader_train, device = device)\n",
    "        print('epoch', epoch, 'train accuracy over whole train set: %d %%' % (accuracy))\n",
    "            \n",
    "        # get the test accuracy\n",
    "        model.eval()\n",
    "        for i, (crops, labels) in enumerate(dataloader_test):\n",
    "            # Get the crops and labels\n",
    "            crops, labels = crops.to(device), labels.to(device)\n",
    "            # get prediction and loss\n",
    "            pred = model(crops.float())\n",
    "            loss = loss_(pred, labels.long())\n",
    "            \n",
    "            running_test_loss += loss.item()\n",
    "            #\n",
    "        print('Confusion matrix from va.idation data')\n",
    "        accuracy = testAccuracy(model, dataloader_test, device=device)\n",
    "        print('epoch', epoch, 'test accuracy over whole validation set: %d %%' % (accuracy))\n",
    "\n",
    "        # save the model if the loss is the best\n",
    "        if (running_test_loss/len(dataloader_test)) < best_loss:\n",
    "            print('Saving model from epoch', epoch)\n",
    "            save_model(model, path)\n",
    "            best_loss = running_test_loss/len(dataloader_test)\n",
    "            patience_counter = 0\n",
    "            best_accuracy = accuracy\n",
    "        elif (running_test_loss/len(dataloader_test)) == best_loss:\n",
    "            if accuracy > best_accuracy:\n",
    "                print('Saving model from epoch', epoch)\n",
    "                save_model(model, path)\n",
    "                best_accuracy = accuracy\n",
    "                patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print('Early stopping at epoch', epoch)\n",
    "                break\n",
    "\n",
    "    \n",
    "        print('Epoch: %d loss: %.3f' % (epoch , running_test_loss / len(dataloader_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "id": "f70bfa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)  # = softmax prob for the true class\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6a9f34d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(percentages_test[i]\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m find_percentage()\n",
      "Cell \u001b[1;32mIn[40], line 7\u001b[0m, in \u001b[0;36mfind_percentage\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m percentages_test \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     total \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(train_data\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m==\u001b[39mi) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(val_data\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m==\u001b[39mi) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(test_data\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m==\u001b[39mi)\n\u001b[0;32m      8\u001b[0m     percentages_train[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(train_data\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m==\u001b[39mi) \u001b[38;5;241m/\u001b[39m total \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      9\u001b[0m     percentages_val[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(val_data\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m==\u001b[39mi) \u001b[38;5;241m/\u001b[39m total \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'labels'"
     ]
    }
   ],
   "source": [
    "# find percentage of each label in the training set\n",
    "def find_percentage():\n",
    "    percentages_train = {}\n",
    "    percentages_val = {}\n",
    "    percentages_test = {}\n",
    "    for i in range(4):\n",
    "        total = torch.sum(train_data.labels==i) + torch.sum(val_data.labels==i) + torch.sum(test_data.labels==i)\n",
    "        percentages_train[i] = torch.sum(train_data.labels==i) / total * 100\n",
    "        percentages_val[i] = torch.sum(val_data.labels==i) / total * 100\n",
    "        percentages_test[i] = torch.sum(test_data.labels==i) / total * 100\n",
    "    \n",
    "    print('Percentage of each label in the training set:')\n",
    "    for i in range(4):\n",
    "        print('label ' + str(i) + ': ' + str(percentages_train[i].item()) + '%')\n",
    "    print('Percentage of each label in the validation set:')\n",
    "    for i in range(4):\n",
    "        print('label ' + str(i) + ': ' + str(percentages_val[i].item()) + '%')\n",
    "    print('Percentage of each label in the test set:')\n",
    "    for i in range(4):\n",
    "        print('label ' + str(i) + ': ' + str(percentages_test[i].item()) + '%')\n",
    "\n",
    "find_percentage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ac1ab351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model, path, dataloader_train, dataloader_val, test_data, samples_weight, device='cuda', num_epochs=100):\n",
    "\n",
    "    # Define the loss functions, and optimizer\n",
    "    #loss = FocalLoss(gamma=2, alpha=torch.tensor(samples_weight).float()) # focal loss function\n",
    "    loss = nn.CrossEntropyLoss(weight=torch.tensor(samples_weight).float()) # cross entropy loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 10, gamma=0.01) # lr=0.1*lr after every 5 epochs\n",
    "\n",
    "    # train the model\n",
    "    train(model, dataloader_train, dataloader_val, loss, num_epochs, path, optimizer, patience=10, scheduler=scheduler,device=device)\n",
    "    \n",
    "    # load best model\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    \n",
    "    # test the model\n",
    "    t_data = torch.clone(test_data.images)\n",
    "    for image in t_data:\n",
    "        image[0,:,:] = image[0,:,:]-torch.mean(image[0,:,:])\n",
    "        image[1,:,:] = image[1,:,:]-torch.mean(image[1,:,:])\n",
    "\n",
    "    y_pred1 = model(t_data)\n",
    "    conf = confusion_matrix(test_data.labels.cpu(), torch.argmax(y_pred1.cpu(), axis=1) )\n",
    "    \n",
    "    print(\"__\"*50)\n",
    "    print(\"Stats for \", path)\n",
    "\n",
    "    print(conf)    \n",
    "\n",
    "    num_classes = conf.shape[0]\n",
    "    precision = np.zeros(num_classes)\n",
    "    recall = np.zeros(num_classes)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        tp = conf[i, i]\n",
    "        fp = np.sum(conf[:, i]) - tp\n",
    "        fn = np.sum(conf[i, :]) - tp\n",
    "        \n",
    "        precision[i] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall[i] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        \n",
    "        print(f\"Class {i}:\")\n",
    "        print(f\"  Precision: {precision[i]:.4f}\")\n",
    "        print(f\"  Recall:    {recall[i]:.4f}\")\n",
    "\n",
    "        # Micro-averaged Precision and Recall\n",
    "        total_tp = np.sum(np.diag(conf))\n",
    "        total_fp = np.sum(conf) - total_tp # Sum of all FPs is sum of all elements minus sum of diagonal\n",
    "        total_fn = np.sum(conf) - total_tp # Sum of all FNs is sum of all elements minus sum of diagonal\n",
    "\n",
    "        micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "        micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "        # Note: For multi-class, micro-precision = micro-recall = micro-F1 = accuracy\n",
    "        print(f\"Micro-averaged Precision: {micro_precision:.4f}\")\n",
    "        print(f\"Micro-averaged Recall:    {micro_recall:.4f}\")\n",
    "\n",
    "\n",
    "    # Overall accuracy (already available from np.sum(np.diag(conf)) / np.sum(conf))\n",
    "    # but can be recalculated for verification\n",
    "    accuracy = np.sum(np.diag(conf)) / np.sum(conf)\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Macro-averaged Precision and Recall\n",
    "    macro_precision = np.mean(precision)\n",
    "    macro_recall = np.mean(recall)\n",
    "    print(f\"Macro-averaged Precision: {macro_precision:.4f}\")\n",
    "    print(f\"Macro-averaged Recall:    {macro_recall:.4f}\")\n",
    "\n",
    "    print(\"__\"*50)\n",
    "\n",
    "    return model, conf, precision, recall, accuracy, macro_precision, macro_recall, micro_precision, micro_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "72e03f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, file_path):\n",
    "    model.load_state_dict(torch.load(file_path, map_location=torch.device('cpu') ) )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1d6c17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of model parameters to try\n",
    "# model = NeuralNetwork(channels=2, crop_size=11, n_outputs=4, fc_layers=2, fc_nodes=200, dropout=0.2).to(device)\n",
    "fc_layers = [2]\n",
    "fc_nodes = [256,512]\n",
    "dropouts = [0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f50ba453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 2 FC layers, 256 nodes, and 0.2 dropout\n",
      "Confusion matrix from train data\n",
      "[[3330  127  559]\n",
      " [ 630 1698  232]\n",
      " [ 549  281  722]]\n",
      "epoch 0 train accuracy over whole train set: 70 %\n",
      "Confusion matrix from test data\n",
      "[[658   5 345]\n",
      " [226 157 209]\n",
      " [137  38 257]]\n",
      "epoch 0 test accuracy over whole validation set: 52 %\n",
      "Saving model from epoch 0\n",
      "Epoch: 0 loss: 1.142\n",
      "Confusion matrix from train data\n",
      "[[3587   23  406]\n",
      " [ 237 2000  323]\n",
      " [ 223  148 1181]]\n",
      "epoch 1 train accuracy over whole train set: 83 %\n",
      "Confusion matrix from test data\n",
      "[[354  12 642]\n",
      " [ 31 150 411]\n",
      " [  1  28 403]]\n",
      "epoch 1 test accuracy over whole validation set: 44 %\n",
      "Epoch: 1 loss: 1.507\n",
      "Confusion matrix from train data\n",
      "[[3336  365  315]\n",
      " [ 123 2278  159]\n",
      " [ 201  396  955]]\n",
      "epoch 2 train accuracy over whole train set: 80 %\n",
      "Confusion matrix from test data\n",
      "[[301  29 678]\n",
      " [  0 265 327]\n",
      " [ 10  32 390]]\n",
      "epoch 2 test accuracy over whole validation set: 47 %\n",
      "Epoch: 2 loss: 1.147\n",
      "Confusion matrix from train data\n",
      "[[3385  151  480]\n",
      " [  43 1537  980]\n",
      " [ 196   94 1262]]\n",
      "epoch 3 train accuracy over whole train set: 76 %\n",
      "Confusion matrix from test data\n",
      "[[474  20 514]\n",
      " [  5 114 473]\n",
      " [ 20  13 399]]\n",
      "epoch 3 test accuracy over whole validation set: 48 %\n",
      "Epoch: 3 loss: 1.665\n",
      "Confusion matrix from train data\n",
      "[[3398  186  432]\n",
      " [  19 2206  335]\n",
      " [  63  128 1361]]\n",
      "epoch 4 train accuracy over whole train set: 85 %\n",
      "Confusion matrix from test data\n",
      "[[406  20 582]\n",
      " [  0 141 451]\n",
      " [ 10  22 400]]\n",
      "epoch 4 test accuracy over whole validation set: 46 %\n",
      "Epoch: 4 loss: 1.832\n",
      "Confusion matrix from train data\n",
      "[[2205  361 1450]\n",
      " [  11 1936  613]\n",
      " [  23   78 1451]]\n",
      "epoch 5 train accuracy over whole train set: 68 %\n",
      "Confusion matrix from test data\n",
      "[[177  13 818]\n",
      " [  0  45 547]\n",
      " [  1  14 417]]\n",
      "epoch 5 test accuracy over whole validation set: 31 %\n",
      "Epoch: 5 loss: 1.760\n",
      "Confusion matrix from train data\n",
      "[[ 778 1024 2214]\n",
      " [  17 1960  583]\n",
      " [ 154  487  911]]\n",
      "epoch 6 train accuracy over whole train set: 44 %\n",
      "Confusion matrix from test data\n",
      "[[323 166 519]\n",
      " [  0 293 299]\n",
      " [ 30  92 310]]\n",
      "epoch 6 test accuracy over whole validation set: 45 %\n",
      "Epoch: 6 loss: 1.191\n",
      "Confusion matrix from train data\n",
      "[[3218   28  770]\n",
      " [  84 2079  397]\n",
      " [  93   97 1362]]\n",
      "epoch 7 train accuracy over whole train set: 81 %\n",
      "Confusion matrix from test data\n",
      "[[286   2 720]\n",
      " [ 21 106 465]\n",
      " [  6  16 410]]\n",
      "epoch 7 test accuracy over whole validation set: 39 %\n",
      "Epoch: 7 loss: 1.548\n",
      "Confusion matrix from train data\n",
      "[[3593   82  341]\n",
      " [  30 2325  205]\n",
      " [ 243  113 1196]]\n",
      "epoch 8 train accuracy over whole train set: 87 %\n",
      "Confusion matrix from test data\n",
      "[[482   4 522]\n",
      " [ 26 181 385]\n",
      " [ 30  19 383]]\n",
      "epoch 8 test accuracy over whole validation set: 51 %\n",
      "Epoch: 8 loss: 1.350\n",
      "Confusion matrix from train data\n",
      "[[3676   61  279]\n",
      " [  16 2378  166]\n",
      " [  86  138 1328]]\n",
      "epoch 9 train accuracy over whole train set: 90 %\n",
      "Confusion matrix from test data\n",
      "[[446   6 556]\n",
      " [  0 154 438]\n",
      " [ 12  15 405]]\n",
      "epoch 9 test accuracy over whole validation set: 49 %\n",
      "Epoch: 9 loss: 1.575\n",
      "Confusion matrix from train data\n",
      "[[3781   27  208]\n",
      " [  23 2374  163]\n",
      " [ 135   83 1334]]\n",
      "epoch 10 train accuracy over whole train set: 92 %\n",
      "Confusion matrix from test data\n",
      "[[450   3 555]\n",
      " [  1 129 462]\n",
      " [ 14  13 405]]\n",
      "epoch 10 test accuracy over whole validation set: 48 %\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nkolev\\AppData\\Local\\Temp\\ipykernel_13408\\1497900860.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Stats for  model_fc2_nodes256_drop0.2_CEloss.pth\n",
      "[[262  18   0]\n",
      " [ 87  65   0]\n",
      " [160   7 129]]\n",
      "Class 0:\n",
      "  Precision: 0.5147\n",
      "  Recall:    0.9357\n",
      "Micro-averaged Precision: 0.6264\n",
      "Micro-averaged Recall:    0.6264\n",
      "Class 1:\n",
      "  Precision: 0.7222\n",
      "  Recall:    0.4276\n",
      "Micro-averaged Precision: 0.6264\n",
      "Micro-averaged Recall:    0.6264\n",
      "Class 2:\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.4358\n",
      "Micro-averaged Precision: 0.6264\n",
      "Micro-averaged Recall:    0.6264\n",
      "\n",
      "Overall Accuracy: 0.6264\n",
      "Macro-averaged Precision: 0.7457\n",
      "Macro-averaged Recall:    0.5997\n",
      "____________________________________________________________________________________________________\n",
      "Training model with 2 FC layers, 512 nodes, and 0.2 dropout\n",
      "Confusion matrix from train data\n",
      "[[3151  353  512]\n",
      " [ 375 1913  272]\n",
      " [ 335  323  894]]\n",
      "epoch 0 train accuracy over whole train set: 73 %\n",
      "Confusion matrix from test data\n",
      "[[290  48 670]\n",
      " [ 54 290 248]\n",
      " [ 34  48 350]]\n",
      "epoch 0 test accuracy over whole validation set: 45 %\n",
      "Saving model from epoch 0\n",
      "Epoch: 0 loss: 1.087\n",
      "Confusion matrix from train data\n",
      "[[3354  153  509]\n",
      " [ 378 1927  255]\n",
      " [ 422  373  757]]\n",
      "epoch 1 train accuracy over whole train set: 74 %\n",
      "Confusion matrix from test data\n",
      "[[495  60 453]\n",
      " [140 185 267]\n",
      " [ 78  75 279]]\n",
      "epoch 1 test accuracy over whole validation set: 47 %\n",
      "Epoch: 1 loss: 1.090\n",
      "Confusion matrix from train data\n",
      "[[3298  107  611]\n",
      " [  23 2305  232]\n",
      " [ 241  319  992]]\n",
      "epoch 2 train accuracy over whole train set: 81 %\n",
      "Confusion matrix from test data\n",
      "[[609  37 362]\n",
      " [ 50 291 251]\n",
      " [ 81  53 298]]\n",
      "epoch 2 test accuracy over whole validation set: 58 %\n",
      "Saving model from epoch 2\n",
      "Epoch: 2 loss: 0.943\n",
      "Confusion matrix from train data\n",
      "[[2037  580 1399]\n",
      " [ 164 1996  400]\n",
      " [ 263  340  949]]\n",
      "epoch 3 train accuracy over whole train set: 61 %\n",
      "Confusion matrix from test data\n",
      "[[384  86 538]\n",
      " [ 51 328 213]\n",
      " [ 33  85 314]]\n",
      "epoch 3 test accuracy over whole validation set: 50 %\n",
      "Epoch: 3 loss: 2.210\n",
      "Confusion matrix from train data\n",
      "[[2032  138 1846]\n",
      " [  28 2041  491]\n",
      " [ 134  464  954]]\n",
      "epoch 4 train accuracy over whole train set: 61 %\n",
      "Confusion matrix from test data\n",
      "[[201  41 766]\n",
      " [ 12 351 229]\n",
      " [ 21  73 338]]\n",
      "epoch 4 test accuracy over whole validation set: 43 %\n",
      "Epoch: 4 loss: 1.077\n",
      "Confusion matrix from train data\n",
      "[[3544  118  354]\n",
      " [  22 2303  235]\n",
      " [ 352  330  870]]\n",
      "epoch 5 train accuracy over whole train set: 82 %\n",
      "Confusion matrix from test data\n",
      "[[525 133 350]\n",
      " [ 20 380 192]\n",
      " [ 48  73 311]]\n",
      "epoch 5 test accuracy over whole validation set: 59 %\n",
      "Epoch: 5 loss: 437.778\n",
      "Confusion matrix from train data\n",
      "[[3259   67  690]\n",
      " [  61 2195  304]\n",
      " [ 224  258 1070]]\n",
      "epoch 6 train accuracy over whole train set: 80 %\n",
      "Confusion matrix from test data\n",
      "[[279  38 691]\n",
      " [ 67 214 311]\n",
      " [ 35  37 360]]\n",
      "epoch 6 test accuracy over whole validation set: 41 %\n",
      "Epoch: 6 loss: 417.943\n",
      "Confusion matrix from train data\n",
      "[[3584   46  386]\n",
      " [  16 2226  318]\n",
      " [ 218  175 1159]]\n",
      "epoch 7 train accuracy over whole train set: 85 %\n",
      "Confusion matrix from test data\n",
      "[[406   9 593]\n",
      " [ 59 234 299]\n",
      " [ 37  21 374]]\n",
      "epoch 7 test accuracy over whole validation set: 49 %\n",
      "Epoch: 7 loss: 242.033\n",
      "Confusion matrix from train data\n",
      "[[3483  102  431]\n",
      " [   9 2400  151]\n",
      " [ 139  312 1101]]\n",
      "epoch 8 train accuracy over whole train set: 85 %\n",
      "Confusion matrix from test data\n",
      "[[423   4 581]\n",
      " [ 39 233 320]\n",
      " [ 22  23 387]]\n",
      "epoch 8 test accuracy over whole validation set: 51 %\n",
      "Epoch: 8 loss: 20.509\n",
      "Confusion matrix from train data\n",
      "[[3787   48  181]\n",
      " [  31 2265  264]\n",
      " [ 322  201 1029]]\n",
      "epoch 9 train accuracy over whole train set: 87 %\n",
      "Confusion matrix from test data\n",
      "[[632   6 370]\n",
      " [ 81 210 301]\n",
      " [ 87  15 330]]\n",
      "epoch 9 test accuracy over whole validation set: 57 %\n",
      "Epoch: 9 loss: 355.884\n",
      "Confusion matrix from train data\n",
      "[[3726   45  245]\n",
      " [  32 2337  191]\n",
      " [ 255  257 1040]]\n",
      "epoch 10 train accuracy over whole train set: 87 %\n",
      "Confusion matrix from test data\n",
      "[[541  11 456]\n",
      " [ 74 247 271]\n",
      " [ 49  18 365]]\n",
      "epoch 10 test accuracy over whole validation set: 56 %\n",
      "Epoch: 10 loss: 104.307\n",
      "Confusion matrix from train data\n",
      "[[3678   45  293]\n",
      " [  16 2347  197]\n",
      " [ 215  267 1070]]\n",
      "epoch 11 train accuracy over whole train set: 87 %\n",
      "Confusion matrix from test data\n",
      "[[513  12 483]\n",
      " [ 55 249 288]\n",
      " [ 37  21 374]]\n",
      "epoch 11 test accuracy over whole validation set: 55 %\n",
      "Epoch: 11 loss: 112.345\n",
      "Confusion matrix from train data\n",
      "[[3678   48  290]\n",
      " [  16 2358  186]\n",
      " [ 187  260 1105]]\n",
      "epoch 12 train accuracy over whole train set: 87 %\n",
      "Confusion matrix from test data\n",
      "[[505  12 491]\n",
      " [ 40 249 303]\n",
      " [ 36  20 376]]\n",
      "epoch 12 test accuracy over whole validation set: 55 %\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nkolev\\AppData\\Local\\Temp\\ipykernel_13408\\1497900860.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Stats for  model_fc2_nodes512_drop0.2_CEloss.pth\n",
      "[[275   0   5]\n",
      " [  0 102  50]\n",
      " [117  12 167]]\n",
      "Class 0:\n",
      "  Precision: 0.7015\n",
      "  Recall:    0.9821\n",
      "Micro-averaged Precision: 0.7473\n",
      "Micro-averaged Recall:    0.7473\n",
      "Class 1:\n",
      "  Precision: 0.8947\n",
      "  Recall:    0.6711\n",
      "Micro-averaged Precision: 0.7473\n",
      "Micro-averaged Recall:    0.7473\n",
      "Class 2:\n",
      "  Precision: 0.7523\n",
      "  Recall:    0.5642\n",
      "Micro-averaged Precision: 0.7473\n",
      "Micro-averaged Recall:    0.7473\n",
      "\n",
      "Overall Accuracy: 0.7473\n",
      "Macro-averaged Precision: 0.7828\n",
      "Macro-averaged Recall:    0.7391\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# dictionary to store the results\n",
    "models_results = {}\n",
    "\n",
    "for fc_layer in fc_layers:\n",
    "    for fc_node in fc_nodes:\n",
    "        for dropout in dropouts:\n",
    "            model = NeuralNetwork(channels=2, crop_size=crop_size-2, n_outputs=3, fc_layers=fc_layer, fc_nodes=fc_node, dropout=dropout).to(device)\n",
    "            path = f\"model_fc{fc_layer}_nodes{fc_node}_drop{dropout}_CEloss.pth\"\n",
    "            print(f\"Training model with {fc_layer} FC layers, {fc_node} nodes, and {dropout} dropout\")\n",
    "            model.to(device)\n",
    "            model, conf, precision, recall, accuracy, macro_precision, macro_recall, micro_precision, micro_recall = train_and_test(model, path, data_loader_train, data_loader_val, test_data,samples_weight,device=device)\n",
    "            models_results[(fc_layer, fc_node, dropout)] = {\n",
    "                \"model\": model,\n",
    "                \"confusion_matrix\": conf,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"macro_precision\": macro_precision,\n",
    "                \"macro_recall\": macro_recall,\n",
    "                \"micro_precision\": micro_precision,\n",
    "                \"micro_recall\": micro_recall\n",
    "            }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c932f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fc_layers</th>\n",
       "      <th>fc_nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>model</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>micro_precision</th>\n",
       "      <th>micro_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NeuralNetwork(\\n  (convolutional_relu_stack): ...</td>\n",
       "      <td>[[264, 5, 3, 8], [0, 145, 7, 0], [68, 30, 144,...</td>\n",
       "      <td>[0.7764705882352941, 0.7880434782608695, 0.832...</td>\n",
       "      <td>[0.9428571428571428, 0.9539473684210527, 0.486...</td>\n",
       "      <td>0.841049</td>\n",
       "      <td>0.823345</td>\n",
       "      <td>0.832178</td>\n",
       "      <td>0.841049</td>\n",
       "      <td>0.841049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fc_layers  fc_nodes  dropout  \\\n",
       "0          2      1024      0.2   \n",
       "\n",
       "                                               model  \\\n",
       "0  NeuralNetwork(\\n  (convolutional_relu_stack): ...   \n",
       "\n",
       "                                    confusion_matrix  \\\n",
       "0  [[264, 5, 3, 8], [0, 145, 7, 0], [68, 30, 144,...   \n",
       "\n",
       "                                           precision  \\\n",
       "0  [0.7764705882352941, 0.7880434782608695, 0.832...   \n",
       "\n",
       "                                              recall  accuracy  \\\n",
       "0  [0.9428571428571428, 0.9539473684210527, 0.486...  0.841049   \n",
       "\n",
       "   macro_precision  macro_recall  micro_precision  micro_recall  \n",
       "0         0.823345      0.832178         0.841049      0.841049  "
      ]
     },
     "execution_count": 1339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put all results into a pandas DataFrame for easy viewing\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(\n",
    "    {k: v for k, v in models_results.items()},\n",
    "   orient='index'\n",
    ")\n",
    "\n",
    "results_df = results_df.reset_index()\n",
    "results_df.columns = [\n",
    "    'fc_layers', 'fc_nodes', 'dropout', 'model', 'confusion_matrix', \n",
    "    'precision', 'recall', 'accuracy', 'macro_precision', \n",
    "    'macro_recall', 'micro_precision', 'micro_recall'\n",
    "]\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1335,
   "id": "1d960e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fc_layers</th>\n",
       "      <th>fc_nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>model</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>micro_precision</th>\n",
       "      <th>micro_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NeuralNetwork(\\n  (convolutional_relu_stack): ...</td>\n",
       "      <td>[[267, 1, 11, 1], [0, 148, 4, 0], [24, 27, 217...</td>\n",
       "      <td>[0.898989898989899, 0.8176795580110497, 0.7587...</td>\n",
       "      <td>[0.9535714285714286, 0.9736842105263158, 0.733...</td>\n",
       "      <td>0.875772</td>\n",
       "      <td>0.855225</td>\n",
       "      <td>0.886482</td>\n",
       "      <td>0.875772</td>\n",
       "      <td>0.875772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NeuralNetwork(\\n  (convolutional_relu_stack): ...</td>\n",
       "      <td>[[271, 0, 9, 0], [0, 130, 22, 0], [25, 14, 239...</td>\n",
       "      <td>[0.8914473684210527, 0.896551724137931, 0.6424...</td>\n",
       "      <td>[0.9678571428571429, 0.8552631578947368, 0.807...</td>\n",
       "      <td>0.846451</td>\n",
       "      <td>0.848144</td>\n",
       "      <td>0.858783</td>\n",
       "      <td>0.846451</td>\n",
       "      <td>0.846451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NeuralNetwork(\\n  (convolutional_relu_stack): ...</td>\n",
       "      <td>[[272, 0, 8, 0], [0, 101, 51, 0], [22, 7, 220,...</td>\n",
       "      <td>[0.9066666666666666, 0.9351851851851852, 0.617...</td>\n",
       "      <td>[0.9714285714285714, 0.6644736842105263, 0.743...</td>\n",
       "      <td>0.831790</td>\n",
       "      <td>0.842871</td>\n",
       "      <td>0.808255</td>\n",
       "      <td>0.831790</td>\n",
       "      <td>0.831790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fc_layers  fc_nodes  dropout  \\\n",
       "0          2       512      0.2   \n",
       "1          2       256      0.2   \n",
       "2          2       128      0.2   \n",
       "\n",
       "                                               model  \\\n",
       "0  NeuralNetwork(\\n  (convolutional_relu_stack): ...   \n",
       "1  NeuralNetwork(\\n  (convolutional_relu_stack): ...   \n",
       "2  NeuralNetwork(\\n  (convolutional_relu_stack): ...   \n",
       "\n",
       "                                    confusion_matrix  \\\n",
       "0  [[267, 1, 11, 1], [0, 148, 4, 0], [24, 27, 217...   \n",
       "1  [[271, 0, 9, 0], [0, 130, 22, 0], [25, 14, 239...   \n",
       "2  [[272, 0, 8, 0], [0, 101, 51, 0], [22, 7, 220,...   \n",
       "\n",
       "                                           precision  \\\n",
       "0  [0.898989898989899, 0.8176795580110497, 0.7587...   \n",
       "1  [0.8914473684210527, 0.896551724137931, 0.6424...   \n",
       "2  [0.9066666666666666, 0.9351851851851852, 0.617...   \n",
       "\n",
       "                                              recall  accuracy  \\\n",
       "0  [0.9535714285714286, 0.9736842105263158, 0.733...  0.875772   \n",
       "1  [0.9678571428571429, 0.8552631578947368, 0.807...  0.846451   \n",
       "2  [0.9714285714285714, 0.6644736842105263, 0.743...  0.831790   \n",
       "\n",
       "   macro_precision  macro_recall  micro_precision  micro_recall  \n",
       "0         0.855225      0.886482         0.875772      0.875772  \n",
       "1         0.848144      0.858783         0.846451      0.846451  \n",
       "2         0.842871      0.808255         0.831790      0.831790  "
      ]
     },
     "execution_count": 1335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = results_df.sort_values(by='micro_precision', ascending=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e28938af-78dd-4c4a-9636-2078954751b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa15d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
